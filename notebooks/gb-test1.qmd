---
format: 
  pdf: 
    documentclass: article
    include-before-body:	
      - text: |
         \begin{titlepage}
         \begin{flushleft}
           { UBMI-IFC, UNAM } \\
           { Coyoacan, CDMX }
         \end{flushleft}
         \vspace*{3cm}
         \begin{center}
           { \Large Sequece Characterization Test } \\[1cm]
           { \large Using Genomic-Benchmarks Data }
         \end{center}
         \vfill
         \begin{flushright}
         \begin{tabular}{l@{\hspace*{\tabcolsep}}l}
           Author: & Fuentes David \\
           Tutors: & \\
           & - PhD. Poot Augusto \\
           & - MSc. Pedraza Carlos \\
         \end{tabular}
         \end{flushright}
         \end{titlepage}
metadata-files: 
  - custom-metadata.yml
mainfont: Liberation Sans
mainfontoptions:
  - Scale=0.9
monofont: JetBrainsMono Nerd Font
monofontoptions:
  - Scale=0.8
execute:
  cache: true
---


\newpage
\setlength\parindent{18pt}
\setlength\columnsep{18pt}
\twocolumn
# Introduction
In this analysis, we explore methods to process, characterize, visualize and classify a preliminary set of activating cis-regulatory
sequences from promoters and enhancers. We started by downloading genomic data, preparing it in a compatible format, and applying 
several custom and existing tools for sequence characterization. The goal was to produce a robust dataset, prepare it for machine 
learning classification, and conduct exploratory analyses that highlight distinctive sequence features. Below, we outline the data 
acquisition, preparation, and characterization steps, as well as the libraries and custom functions employed.

\vspace{0.22cm}
\begin{notebox}[center, width=0.95\columnwidth]{Note:}
Each time the programming language changes, the code-cell will 
display a header indicating the corresponding language.
\end{notebox}

# Data Preparation
## Downloading data
Although the initial intention was to use sequences from various \break public databases (like \impw{GeneHancer}, 
\impw{RefSeq}, \impw{ENCODE} and \impw{EPD} among others), it was suggested to run some tests with the easy-access 
data from \impw{genomic-benchmarks} (*Grešová et al., 2023*), so the first step was to explore all available datasets 
after downloading the corresponding \impw{python} package through '\fn{pip install genomic-benchmarks}'. 

After that, we have to '\fn{import}' the needed functions to explore the available datasets. It feels noteworthy 
to mention the importance of internet connectivity as a determining factor of the run-times of the following functions.

\vspace{0.33cm}
\begin{pythonheader}
Python Code
\end{pythonheader}
\vspace{-1.75pt}
```{python}
#| label: genomic-benchmarks-libraries
#| cache: false

# To list available datasets
from genomic_benchmarks.data_check import \
	list_datasets

# To inspect each dataset to select two
from genomic_benchmarks.data_check import info as \
	info_gb

# To download each dataset
from genomic_benchmarks.loc2seq import \
	download_dataset

# To position ourselves in the correct directory
import os
```

Due to some cell-output inconveniences explained briefly in the \impw{\textit{Custom Functions} 
Annex} and a personal preference for modularity, next we will import '\fn{custom\_functions}'. 
\fontlibertser{\small Refer to 'Custom Functions Annex', Section 1, for more details}.

\vspace{0.25cm}
```{python}
#| label: custom-modules
#| echo: false
#| cache: false

# To load the 'custom-functions' module
os.chdir("/home/davidfm/Projects/UBMI-IFC/EnhaProm/scripts")
from custom_functions import *
```
```{python}
#| label: custom-modules-facade
#| eval: false
#| cache: false

# To load the 'custom-functions' module
os.chdir("/path/to/Project/scripts")
from custom_functions import *
```

When displaying the available datasets, we can highlight the presence of four datasets containing \impw{human} 
regulatory elements: '\impw{\textit{non-TATA} promoters}', '\impw{enhancers \textit{Ensembl}}', '\impw{enhancers 
\textit{Cohn}}' and '\impw{\textit{Ensembl} regulatory}':

\vspace{0.25cm}
```{python}
#| label: data-skimming-false
#| eval: false
list_datasets()
```
```{python}
#| label: data-skimming 
#| echo: false
#| cache: false

outputwrap1(list_datasets(), set_width = 65)
# EquivalenT to: list_datasets()
```

\vspace{0.25cm}
```{python}
#| label: dataset1-exploration-facade
#| eval: false

info_gb("human_nontata_promoters", version=0)
```
```{python}
#| label: dataset1-exploration
#| echo: false

# Equal to: 
# info_gb("human_nontata_promoters", version=0)
outputwrap(info_gb, 
		   args=("human_nontata_promoters",),
		   kwargs={"version": 0}, width=50) 
```

\vspace{0.25cm}
```{python}
#| label: dataset2-exploration-facade
#| eval: false

info_gb("human_ensembl_regulatory", version=0)
```
```{python}
#| label: dataset2-exploration
#| echo: false

# Equal to: 
# info_gb("human_ensembl_regulatory", version=0)
outputwrap(info_gb, 
		   args=("human_ensembl_regulatory",),
		   kwargs={"version": 0}, width=50)
```

\vspace{0.25cm}
```{python}
#| label: dataset3-exploration-facade
#| eval: false

info_gb("human_enhancers_cohn", version=0)
```
```{python}
#| label: dataset3-exploration
#| echo: false

# Equal to: 
# info_gb("human_enhancers_cohn", version=0)
outputwrap(info_gb, 
		   args=("human_enhancers_cohn",),
		   kwargs={"version": 0}, width=50) 
```

\vspace{0.25cm}
```{python}
#| label: dataset4-exploration-facade
#| eval: false

info_gb("human_enhancers_ensembl", version=0)
```
```{python}
#| label: dataset4-exploration
#| echo: false

# Equal to: 
# info_gb("human_enhancers_ensembl", version=0)
outputwrap(info_gb, args=("human_enhancers_ensembl",),
			kwargs={"version": 0}, width=50)
```

'OCR Ensembl':
\vspace{0.5cm}
```{python}
#| label: dataset5-exploration-facade
#| eval: false

info_gb("human_ocr_ensembl", version=0)
```
```{python}
#| label: dataset5-exploration
#| echo: false

# Equal to: 
# info_gb("human_enhancers_ensembl", version=0)
outputwrap(info_gb, args=("human_ocr_ensembl",),
			kwargs={"version": 0}, width=50)
```

\vspace{0.25cm}
```{python}
#| label: data-download-facade
#| eval: false

os.chdir("/path/to/Project/datasets/GenomicBenchmarks")

download_dataset("human_nontata_promoters", version=0) 
download_dataset("human_enhancers_cohn", version=0) 
```
```{python}
#| label: data-download
#| echo: false
#| eval: false

os.chdir("/home/davidfm/Projects/UBMI-IFC/EnhaProm/datasets/GenomicBenchmarks")
download_dataset("human_nontata_promoters", version=0) 
download_dataset("human_enhancers_cohn", version=0) 
```

## Formatting data
The downloaded data consisted of multiple '*.txt*' files organized into two directories, and since at least the 
'*getShape()*' function from the '*DNAshapeR*' package required FASTA files to work, it felt right to integrate 
all sequences of each cis-regulatory element in a single FASTA file. For this, I gave them all a simple header 
and appended them together with *AWK*.

\begin{bashheader}
Bash Code
\end{bashheader}
\vspace{-1.75pt}
```{bash}
#| label: data-to-fasta-facade
#| eval: false

cd /path/to/Project/datasets/GenomicBenchmarks/

awk 'BEGIN{counter=0}
     {print ">promoter_"counter"|train|positive";
     print $0; counter+=1}' \
     human_nontata_promoters/train/positive/*.txt \
     > promoters_train_positive.fasta

awk 'BEGIN{counter=0}
     {print ">enhancer_"counter"|train|positive"; 
     print $0; counter+=1}' \
     human_enhancers_cohn/train/positive/*.txt \
     > enhancers_train_positive.fasta
```

```{bash}
#| label: data-to-fasta
#| eval: false
#| echo: false

cd /home/davidfm/Projects/UBMI-IFC/EnhaProm/datasets/GenomicBenchmarks/
awk 'BEGIN{counter=0}{print ">promoter_"counter"|train|positive";
     print $0; counter+=1}' human_nontata_promoters/train/positive/*.txt \
     > promoters_train_positive.fasta
awk 'BEGIN{counter=0}{print ">enhancer_"counter"|train|positive"; 
     print $0; counter+=1}' human_enhancers_cohn/train/positive/*.txt \
     > enhancers_train_positive.fasta
```

# Data Characterization
## Libraries used
The following procedures require several R libraries, alongside custom functions 
developed for sequence characterization. The libraries and their respective roles 
in the analysis are outlined below:

\begin{rheader}
R Code
\end{rheader}
\vspace{-1.75pt}
```{r}
#| label: requirements
#| include: false
#| cache: false

# For genome-functions.R
library(stringr)
library(stringi)
library(primes)
# For parallel computing
library(doParallel)
library(foreach)
# For biological functions: 
#   - Local/Global alignments 
#   - DNA Shape computing
library(Biostrings)
library(DNAshapeR)
# For plotting
library(paletteer)
library(cowplot)
library(ggplot2)
library(dplyr)
library(plyr)
library(see) # for 'geom_violinhalf'
# For pretty tables
library(knitr)
library(kableExtra)
# For my own functions
source("/home/davidfm/Projects/UBMI-IFC/EnhaProm/scripts/genome-functions.R")
source("/home/davidfm/Projects/UBMI-IFC/EnhaProm/scripts/custom-functions.R")
```

```{r}
#| label: requirements-facade
#| eval: false

# For useful tools like 'filter'
library(dplyr)
library(plyr)
# For genome-functions.R
library(stringr)
library(stringi)
library(primes)
# For parallel computing
library(doParallel)
library(foreach)
# For biological functions: 
library(Biostrings)	# Local/Global Alignments
library(DNAshapeR)	# DNA Shape Features
# For plotting
library(paletteer)	# Color Palettes
library(cowplot)	# Plot Grids
library(ggplot2)
library(see)		# Half Violin Plots
# For pretty tables
library(knitr)
library(kableExtra)
# For my own functions
source("/path/to/Project/scripts/genome-functions.R")
source("/path/to/Project/scripts/custom-functions.R")
```


## Characterizing sequences
Here, we characterize a subset from each of our datasets: 1638 sequences per 
regulatory element; 3276 in total. However, there's a circumstance about the 
sequences that has to be noted:

* Both datasets have considerably different elements lengthwise:

	1. All promoters have a length of 251 nucleotides.

    2. All enhancers have a length of 500 nucleotides.

* All features per sequence must be numerical and two-dimensional since we want
it to be fed eventually to a simple classifier (like a Support Vector Machine).

```{r}
#| label: csv-scanning
#| eval: false
#| echo: false

# Scanning sequences
prom_fasta <- "datasets/GenomicBenchmarks/promoters_train_positive.fasta"
enha_fasta <- "datasets/GenomicBenchmarks/enhancers_train_positive.fasta"
prom_seqs <- scan(prom_fasta, character(), quote = "")[seq(2, 29484, 2)]
enha_seqs <- scan(enha_fasta, character(), quote = "")[seq(2, 20842, 2)]
```

```{r}
#| label: csv-scanning-facade
#| eval: false

proj_path <- "path/to/Project/datasets/GenomicBenchmarks"
prom_fastaname <- "promoters_train_positive.fasta"
enha_fastaname <- "enhancers_train_positive.fasta"

prom_path <- paste(proj_path, prom_fastaname, sep = "/")
enha_path <- paste(proj_path, enha_fastaname, sep = "/")

# Scanning sequences
prom_seqs <- scan(prom_path, 
                  character(), quote="")[seq(2,29484,2)]
enha_seqs <- scan(enha_path, 
                  character(), quote="")[seq(2,20842,2)]
```

Given some previous tests done to 'sequences_characterizer()' I came to the
conclusion that parallel computing might provide a higher and more complex
set of data in a feasible time span.

\vspace{0.5cm}
```{r}
#| label: csv-construction
#| eval: false
#| echo: false

# Prepairing clusters for parallel computing
corescluster <- makeCluster(6)
registerDoParallel(corescluster)
# Characterizing sequences and exporting to CSV
list_seqs <- list(promoters = prom_seqs, enhancers = enha_seqs)
reg_elems <- c("promoters", "enhancers")
for (reg_elem in reg_elems) {
  foreach(i = 1:6) %dopar% {
    library(stringr) # for some reason we have to specify for
    library(stringi) # all the libraries that the sequence
    library(primes)  # characterizer needs. NOTE: Try to put them all inside a function
    i_start <- ((i - 1) * 273) + 1
    i_final <- i * 273
    if (i > 1) {     # Conditional so that only the first CSV has headers
      write.table(sequences_characterizer(list_seqs[[reg_elem]][i_start:i_final],
                                          k_max = 6, optim = TRUE),
                  paste("datasets/GB-Testing/test", reg_elem, "-minitraining_",
                        i, ".csv", sep = ""), sep = ",",
                  row.names = FALSE, col.names = FALSE)
    } else {
	  write.csv(sequences_characterizer(list_seqs[[reg_elem]][i_start:i_final],
                                        k_max = 6, optim = TRUE),
                paste("datasets/GB-Testing/", reg_elem, "-minitraining_",
                      i, ".csv", sep = ""), row.names = FALSE)
    }
  }
}
```

```{r}
#| label: csv-construction-facade
#| eval: false

# Prepairing clusters for parallel computing
corescluster <- makeCluster(6)
registerDoParallel(corescluster)

# Characterizing sequences and exporting to CSV
list_seqs <- list(promoters = prom_seqs,
                  enhancers = enha_seqs)
reg_elems <- c("promoters", "enhancers")
libs <- c("stringr", "stringi", "primes")

for (reg_elem in reg_elems) {
  foreach(i = 1:6) %dopar% {
    required_libs(libs)
    i_start <- ((i - 1) * 273) + 1
    i_final <- i * 273

    if (i > 1) { # Only the first CSV has headers
      write.table(
        sequences_characterizer(
          list_seqs[[reg_elem]][i_start:i_final],
          optim = TRUE, k_max = 6),
        paste(
          "datasets/GB-Testing/test", reg_elem, 
          "-training_", i, ".csv", sep = ""), 
          row.names = FALSE, col.names = FALSE,
          sep = ",")

    } else {
	  write.csv(
        sequences_characterizer(
          list_seqs[[reg_elem]][i_start:i_final],
          optim = TRUE, k_max = 6),
        paste(
          "datasets/GB-Testing/", reg_elem,
          "-training_", i, ".csv", sep = ""),
          row.names = FALSE)
}}}
```

## Concatenating CSV's
It was decided to produce many files instead of appending over the same CSV table in order to 
apply a sort of quality control checkup after the parallel computing was done. This, because when forcing the process 
RAM overload was possible and the function could die mid-process. Since our data was separated in six tables per 
cis-regulatory element, here we only join each set together in a single CSV.

\begin{bashheader}
Bash Code
\end{bashheader}
\vspace{-1.75pt}
```{bash}
#| label: csv-concatenation
#| eval: false
#| echo: false

# Only added this cell in case I forget the original file names
cat datasets/GB-Testing/testpromoters-minitraining_*.csv \
    > datasets/GB-Testing/test-1638-promoters-6mers.csv
cat datasets/GB-Testing/testenhancers-minitraining_*.csv \
    > datasets/GB-Testing/test-1638-enhancers-6mers.csv
```

```{bash}
#| label: csv-concatenation-facade
#| eval: false

cat datasets/GB-Testing/testpromoters-training_*.csv \
    > datasets/GB-Testing/test-1638-promoters-6mers.csv
cat datasets/GB-Testing/testenhancers-training_*.csv \
    > datasets/GB-Testing/test-1638-enhancers-6mers.csv
```

## Data description

\begin{table}[h]
\caption{Overview of each column generated so far by '\textit{sequences-characterizer()}'}
\small
\centering
% Using the new column types L, C, and R for custom width and orientation
\begin{tabular}{|L{1.5cm}|C{2.5cm}|C{4cm}|} 
\hline
\centering \textbf{Column} & \textbf{Description} & \textbf{Section Processed} \\
\hline
A & \parbox[c][1.2cm]{2.5cm}{\centering Percentage of Alanines} & \multirow{6}{*}{\underline{per Sequence}} \\
\cdashline{1-2}
T & \parbox[c][1.2cm]{2.5cm}{\centering Percentage of Thymines} & \\
\cdashline{1-2}
C & \parbox[c][1.2cm]{2.5cm}{\centering Percentage of Cytosines} & \\
\cdashline{1-2}
G & \parbox[c][1.2cm]{2.5cm}{\centering Percentage of Guanines} & \\
\cdashline{1-2}
temp & \parbox[c][1.2cm]{2.5cm}{\centering Melting Temperature} & \\
\cdashline{1-2}
shan & \parbox[c][1.2cm]{2.5cm}{\centering Shannon Coefficient} & \\
\hline
kN.M\_prod & \parbox[c][1.25cm]{2.5cm}{\centering KSG Product} & \multirow{4}{*}{\parbox[t]{4cm}{
\setlength\parindent{35pt}\underline{per Kmer:}\\\\
each possible kmer of \\ size \textit{N}
is identified on a \\ scale of 1 to
$4^N$ denoted \\ by \textit{M}.\\\\
\texttt{
i.e. k3.1 (or the first\\
kmer of size 3) would be\\
AAA; k4.2 would be AAAC\\
}
}} \\
\cdashline{1-2}
kN.M\_barc & \parbox[c][1.2cm]{2.5cm}{\centering Barcode Profile} & \\
\cdashline{1-2}
kN.M\_pals & \parbox[c][1.2cm]{2.5cm}{\centering Palindrome Profile} & \\
\cdashline{1-2}
kN.M\_revc & \parbox[c][1.7cm]{2.5cm}{\centering Reverse Complement Profile} & \\
\hline
\end{tabular}
\end{table}

Prior to our primary analysis, it feels reasonable to explain the columns per sequence produced by our
tabulator '*sequences_characterizer()*'. From here we'll first describe the ones computed sequence-wise, 
the ones computed kmer-wise, then the ones computed over each kmer distribution, and finally the ones 
corresponding to DNA-Shape; this feature comes at last considering '*getShape()*' function makes its' own 
dataframe. The ones in **black** are already integrated in the table, the ones in \color{red} **red** 
\color{black} are yet to be adjoined:

\underline{Per sequence}

* From *'genome-functions.R'*:
    * **A, T, C, G** - *Nucleotide Percentages* per sequence.

    * **temp** - \textit{Melting \underline{Temp}erature}: Temperature at which DNA's double helix dissociates 
	into single strands. *It's dependent on GC percentage and sequence length.*

    * **shan** - \textit{\underline{Shan}non Entropy Coefficient}: Statistical quantifier of information in a 
    system. Measures the uncertainty a set of data has. In this case is a nucleotide-diversity metric. 
    *It's dependent on nucleotide percentages.*

* From *'Biostrings'*:
	* \color{red}**la_sc** \color{black} - \textit{\underline{L}ocal \underline{A}lignment \underline{Sc}ore}

	* \color{red}**la_id** \color{black} - \textit{\underline{L}ocal \underline{A}lignment \underline{Id}entity}

	* \color{red}**ga_sc** \color{black} - \textit{\underline{G}lobal \underline{A}lignment \underline{Sc}ore}

	* \color{red}**ga_id** \color{black} - \textit{\underline{G}lobal \underline{A}lignment \underline{Id}entity}


\underline{Per kmer}

Kmer characterization came with many challenges, principally to provide each kmer with a distinct signature 
value dependent on their sequence structure. This under the assumption that .

* From *'genome-functions.R'*:
	* **kN.M_prod** - \textit{KSG \underline{Prod}uct}: Its obtained by multiplying \fontnimbussnn{\textbf{
	\underline{K}mer-Percentage * \underline{S}hannon Entropy * \underline{G}C-Percentage}} (*in concept*). 
	The justification for this comes from the desire to give each kmer a distictive signal based on their 
	sequence. However many kmers can produce the same value depending on the function, for example, the kmers 
	\fontnimbussnn{\textbf{'AAAT', 'ATAA', 'CCGC'}} and \fontnimbussnn{\textbf{'TGGG'}} all have the *same Entropy 
	Coefficient* (0.811), while \fontnimbussnn{\textbf{'GGAT', 'CAAG', 'AACC'}} and \fontnimbussnn{\textbf{'TGCT'}} 
	all have the *same GC Percentage* (0.5). Additionally, it is a little tricky to *keep the product from becoming 
	zero* when the sequence is either devoid of C/G nucleotides (e.g. \fontnimbussnn{\textbf{gc\_percentage('ATTA')
	=0}}), or lacks nucleotide diversity (e.g. \fontnimbussnn{\textbf{shannon\_entropy('GGGG')=0}}). 
	\newline\small\fontlibertser{Refer to 'Theory \& Code Annex', Sections 2 \& 3, for more details.}\normalsize

    * **kN.M_barc** - \textit{\underline{Barc}ode Profile}: Its obtained by generating a vector with size equal to
	the kmer amount (which is equivalent to \fontnimbussnn{\textbf{'sequence length'} - \textbf{'kmer length'} + 1})
	and filling it with one prime number per cell in ascending order. Afterwars we use a logical (\fontnimbussnn{
	\textbf{1's \& 0's}}) vector of the same size to represent each kmer's positions in the sequence. *In concept*, 
	if we multiply both vectors we will get a set of prime numbers representing the positions of each kmer inside 
	the sequence. If we multiply as well the elements inside this set, we will obtain an exlusive value for each 
	kmer in each sequence, and only the sequences with the same kmers in the same place will be divisible by the 
	corresponding prime.

    * **kN.M_pals** - \textit{\underline{Pal}indrome\underline{s}' Profile}:

    * **kN.M_revc** - \textit{\underline{Rev}erse \underline{C}omplement Profile}:

\underline{Per kmer distribution}

\underline{Per non-defined kmer}

* From *'DNAshapeR'*: \newline Generates a set features as vectors (EP, MGW, HelT, Roll & ProT by default) per provided 
FASTA file. Splits each sequence in sliding kmers of size 5 (pentamers) not defined by me, which therefore makes vector 
size dependent on sequence-length. The features intended to be used are:

	* \color{red}**sh_ep** \color{black} - Shape EP

	* \color{red}**sh_mgw** \color{black} - \textit{\underline{M}inor \underline{G}roove \underline{W}idth}

	* \color{red}**sh_helt** \color{black} - \textit{\underline{Hel}ix \underline{T}wist}

	* \color{red}**sh_prot** \color{black} - \textit{\underline{Pro}peller \underline{T}wist}

	* \color{red}**sh_roll** \color{black} - \textit{\underline{Roll}}

# Exploration analysis
## Primary analysis
Firstly we read our CSV tables into our conviniently named dataframes.

We could have concatenated both tables together, however I prefer to keep them in different files.
\begin{rheader}
R Code
\end{rheader}
\vspace{-1.75pt}
```{r}
#| label: data-load
#| echo: false

setwd("/home/davidfm/Projects/UBMI-IFC/EnhaProm")
proms <- read.csv("datasets/GB-Testing/test-1638-promoters-6mers.csv", 
                  check.names = F)
enhas <- read.csv("datasets/GB-Testing/test-1638-enhancers-6mers.csv", 
                  check.names = F)
CREs <- c("Promoter","Enhancer")
```

```{r}
#| label: data-load-facade
#| eval: false

setwd("/path/to/Project/")
testdir_path <- "datasets/GB-Testing/"
prom_csvpath <- "test-1638-promoters-6mers.csv" 
enha_csvname <- "test-1638-enhancers-6mers.csv" 

proms <- read.csv(paste0(testdir_path, 
                         prom_csvname),
                         check.names = F)
enhas <- read.csv(paste0(testdir_path, 
                         enha_csvname),
                         check.names = F)
CREs <- c("Promoter","Enhancer")
```
\vspace{0.2cm}

First we get an overviwew of the dimensions of our data:
NOTE: Replace this with some kind of table
\vspace{0.2cm}
```{r}
#| label: data-dimensions
#| layout-ncol: 2
deparse(substitute(proms))
deparse(substitute(enhas))
dim(proms)
dim(enhas)
```
\vspace{0.2cm}

It's noticeable the fact that we have way more columns than rows in this test table.
Let's get a general glimpse of the first three promoters' rows by displaying the first 
3 lines and first 18 columns pertaining to all *sequence-wise* variables computed so far 
plus the first 12 *kmer-wise* variables, which align with the data related to the first 
3 kmers (\fontnimbussnn{\textbf{'AA', 'AC'} \& \textbf{'AG'}}):

\vspace{0.2cm}
```{r}
#| label: head-prom-facade
#| eval: false

teatable(proms[1:3,1:18])
```

```{r}
#| label: head-prom
#| echo: false
#| cache: false

# Each time teatable is used, its' cell must be reloaded
# teatable(proms[1:3,1:6], colsize = "6em")
#   # kableExtra::add_header_above(header=c("Test Promoters"=6))
# teatable(proms[1:3,7:12], colsize = "6em")
# teatable(proms[1:3,13:18], colsize = "6em")
teatable(proms[1:3,1:6], colsize = "5.75em")
teatable(proms[1:3,7:12], colsize = "5.75em")
teatable(proms[1:3,13:18], colsize = "5.75em")
```
\vspace{0.2cm}

We'll get a similar panorama when looking at the first three enhancers' rows
(although it seems like *'barcode'* values appear to be significantly larger):

\vspace{0.2cm}
```{r}
#| label: head-enha-facade
#| eval: false

teatable(enhas[1:3,1:18])
```
```{r}
#| label: head-enha
#| echo: false
#| cache: false

teatable(enhas[1:3,1:6], colsize = "6em") #|>
  # kableExtra::add_header_above(header=c("Test Enhancers"=6))
teatable(enhas[1:3,7:12], colsize = "6em")
teatable(enhas[1:3,13:18], colsize = "6em")
```

```{r}
#| label: data-summary
#| cache: false

mean_prom <- colMeans(proms)
mean_enha <- colMeans(enhas)
sd_prom <- apply(proms, 2, sd)
sd_enha <- apply(enhas, 2, sd)

names_CREs <- rep(CREs, each = length(proms))
cre_summary <- data.frame(Type = factor(names_CREs),
                          Field = rep(names(proms),2),
                          Means = c(mean_prom,mean_enha),
                          StDevs = c(sd_prom,sd_enha))
```

```{r}
#| label: summary-head
#| cache: false

ldf <- length(proms)
coffeetable(cre_summary[c(1:10,(ldf+1):(ldf+10)),])
```

This text should be later deleted
```{r}
#| layout-nrow: 3
#| code-overflow: wrap
# Get only 'prod' columns of each kmer

k_Ns <- c(n_ki(2), n_ki(3), n_ki(4), n_ki(5), n_ki(6)) 
k_inds <- c(0)
for (k_n in k_Ns) {
  k_inds <- c(k_inds, tail(k_inds,1) + 1)
  k_inds <- c(k_inds, tail(k_inds,2)[1] + k_n)
}
k_inds <- k_inds[-1]
# print(rev(k_inds)[1])
# print((ldf-6)/4)
# print(ldf)
```

```{r}
#| label: kmer-set-sizes
#| crop: true
#| fig-height: 7
#| fig-align: center

kmer_set_sizes <- data.frame(sizes = k_Ns,
  k_sets = c("2","3","4","5","6"))

ggplot(kmer_set_sizes, aes(x = "", y = sizes,
                           fill = k_sets))+ 
  geom_bar(width = 1, stat = "identity") + 
  labs(y = "Count", x = "", 
       title = "Kmers per Kmer Size", 
       fill = "k =") +
  coord_polar("y", start = 0) +
  theme_minimal() +
  scale_fill_paletteer_d("PNWColors::Bay") +
  theme(legend.position = "right", 
        axis.title = element_text(size = 15),
        text = element_text(size = rel(4.25)),
        legend.text = element_text(size = 13),
        legend.title = element_text(size = 13.5),
        axis.text.y = element_text(family = "mono"),
        plot.title = element_text(size = 16, hjust = 0.5))
  
```

```{r}
#| label: indexes

pe_arr <- function(seq_data)
  return(array(seq_data, dim = c(rev(k_inds)[1], 2),
    dimnames = list(1:rev(k_inds)[1], c("prom","enha"))))

indxs <- list(
  prod = pe_arr(c(seq(7,ldf-3,4), ldf+seq(7,ldf-3,4))),
  barc = pe_arr(c(seq(8,ldf-2,4), ldf+seq(8,ldf-2,4))),
  pals = pe_arr(c(seq(9,ldf-1,4), ldf+seq(9,ldf-1,4))),
  revc = pe_arr(c(seq(10,ldf,4), ldf+seq(10,ldf,4))))
```


```{r}
#| label: visual-table
#| echo: false
#| eval: false
# This is just a neat table I liked
# ...don't really know why
cols_names <- colnames(subset_cre_prod)
pcol_names <- paste0(cols_names, ".P")
ecol_names <- paste0(cols_names, ".E")
pcol_names[2] <- cols_names[2]
newc_names <- c(pcol_names, ecol_names)
cols_order <- c(2,1,3,4,5,7,8)
visual_table <- head(cbind(
  filter(subset_cre_prod, Type == "Promoter"),
  filter(subset_cre_prod, Type == "Enhancer")
              )[, cols_order], 10)
colnames(visual_table) <- newc_names[cols_order]
coffeetable(visual_table, row.names = FALSE)
```

```{r}
#| label: figure-nucl
#| row: screen
#| fig-height: 7
#| crop: true
#| fig-align: center

# Comment
pyrplot_(cre_summary[c(1:4, ldf + (1:4)), ], 
      x_label = "Nucleotides", y_breaks = seq(-1,1,0.2), 
      title = "Percentage Means per Nucleotide")
```

```{r}
#| label: subset-temp-shan

data_e <- cbind(Type = rep("Enhancer",
                length(enhas$temp)), enhas[,5:6])
data_p <- cbind(Type = rep("Promoter",
                length(proms$temp)), proms[,5:6])
subset_tm_sh <- rbind(data_e, data_p)
```

```{r}
#| label: subset-temp-shan-3
#| row: screen
#| crop: true
#| fig-height: 7
#| fig-align: center

# Comment
# Saving Melting Temperature Violin Plot 
tm_violin <- hvioplot_(data = subset_tm_sh, 
                       y_var = "temp", 
                       y_label = "Density")

# Saving Shannon Violin Plot 
sh_violin <- hvioplot_(data = subset_tm_sh, 
                       y_var = "shan", 
                       fill_legend_title = "CRE Type")

# Get legend
legend_violin <- get_legend_bypass(sh_violin +
  guides(color = guide_legend(nrow = 1)) +
  theme(legend.position = "bottom",
        legend.title = element_text(size = 13.5),
        legend.text = element_text(size = 13)))

# Merging of Violin-Plots 
grid_violin <- plot_grid(tm_violin, sh_violin, NULL,
                 ncol = 3, rel_widths = c(1,1,0.1),
                 labels = c("Melting Temperature",
                 "Shannon Entropy"), label_size = 16,
                 label_fontface = "plain", vjust = 1,
                 hjust = c(-0.38, -0.48))

# Adding legend at the bottom
plot_grid(grid_violin, legend_violin,
          ncol = 1, rel_heights = c(12,1))
```

```{r}
#| label: figure-shan-temp
#| row: screen
#| crop: true
#| fig-height: 7
#| fig-align: center

# Comment
# Saving Melting Temperature Bar-Plot
temp_plot <- barplot_(filter(cre_summary, Field=="temp"), 
                      y_breaks = seq(0, 100, 10), 
                      y_axis_title = "Means")

# Saving Shannon Coefficient Bar-Plot
shan_plot <- barplot_(filter(cre_summary, Field=="shan"), 
                      y_breaks = seq(0, 2, 0.25), 
                      fill_legend_title = "CRE Type")

# Merging of Bar-Plots
plot_grid(temp_plot, shan_plot,  
    ncol = 2, rel_widths = c(1,1), 
    vjust = 1, hjust = c(-0.35, -0.48),
    label_size = 16, label_fontface = "plain",
    labels = c("Melting Temperature", "Shannon Entropy")) 
```


```{r}
#| label: figure-prods
#| fig-height: 8
#| row: screen
#| fig-align: center

kmer_names <- combi_kmers(k=3)[1:48]

pyrplot_(cre_summary[indxs$prod[17:64,],], kmer_names,
         x_label = "Kmers", y_breaks = seq(-30,30,10), 
         title = "KSG-Product Means per Kmer")

pyrplot_(cre_summary[indxs$barc[17:64,],], kmer_names,
         x_label = "Kmers", y_breaks = seq(-10,10,1), 
         title = "Barcode Profile Means per Kmer")
```



