---
format: 
  pdf: 
    documentclass: article
    include-before-body:	
      - text: |
         \begin{titlepage}
         \begin{flushleft}
           { UBMI-IFC, UNAM } \\
           { Coyoacan, CDMX }
         \end{flushleft}
         \vspace*{3cm}
         \begin{center}
           { \Large Sequence Characterization Test 2} \\[1cm]
           { \large Using Genomic-Benchmarks Data }
         \end{center}
         \vfill
         \begin{flushright}
         \begin{tabular}{l@{\hspace*{\tabcolsep}}l}
           Author: & Fuentes David \\
           Tutors: & \\
           & - PhD. Poot Augusto \\
           & - MSc. Pedraza Carlos \\
         \end{tabular}
         \end{flushright}
         \end{titlepage}
metadata-files: 
  - custom-metadata.yml
mainfont: Liberation Sans
mainfontoptions:
  - Scale=0.8
monofont: JetBrainsMono Nerd Font
monofontoptions:
  - Scale=0.7
execute:
  cache: true
---


\newpage
\setlength\parindent{18pt}
\setlength\columnsep{18pt}
\twocolumn

# Data Preparation
## Downloading data

\vspace{0.33cm}
\begin{pythonheader}
Python Code
\end{pythonheader}
\vspace{-1.75pt}
```{python}
#| label: python-libraries

# To inspect each dataset to select two
from genomic_benchmarks.data_check import info as \
	info_gb

# To download each dataset
from genomic_benchmarks.loc2seq import \
	download_dataset

import os
```

```{python}
#| label: custom-modules-facade
#| eval: false

# To load the 'custom-functions' module
os.chdir("/path/to/Project/scripts")
from custom_functions import *
```
```{python}
#| label: custom-modules
#| echo: false

# To load the 'custom-functions' module
os.chdir("/home/davidfm/Projects/EnhaProm/scripts")
from custom_functions import *
```

\vspace{0.25cm}
```{python}
#| label: dataset-exploration-facade
#| eval: false

info_gb("human_ensembl_regulatory", version=0)
```
```{python}
#| label: dataset-exploration
#| echo: false

# Equal to: 
# info_gb("human_ensembl_regulatory", version=0)
outputwrap(info_gb, 
		   args=("human_ensembl_regulatory",),
		   kwargs={"version": 0}, width=50)
```

\vspace{0.25cm}
```{python}
#| label: data-download-facade
#| eval: false

download_dataset("human_ensembl_regulatory", version=0) 
```

## Formatting data
Concatenate sequences in '.txt' files into a single FASTA (this is a temporary solution since the FASTA has "ugly" headers (counter doesn't work))
'find' + 'xargs' approach used since awk couldn't handle >80K files at once

\vspace{0.33cm}
\begin{bashheader}
Bash Code
\end{bashheader}
\vspace{-1.75pt}
```{bash}
#| label: fasta-formatting
#| eval: false
cd /path/to/Project/datasets
mkdir GenomicBenchmarks

# Move sequences to our project datasets directory
mv ~/.genomic_benchmarks/human_ensembl_regulatory \
   ./GenomicBenchmarks

elements=("enhancer" "promoter" "OCR")
data_path="GenomicBenchmarks/human_ensembl_regulatory/train"

# Concatenates sequence 'txt' files into single FASTA
# NOTE: '${variable,,}' changes string 'variable' to lowercase
for element in "${elements[@]}"; do
  find "${data_path}/${element,,}/" -type f -name '*.txt' | \
  xargs -I {} awk -v prefix="${element,,}" \
        'BEGIN{counter=0} {print ">"prefix"_"counter"|training"; 
         print $0; counter+=1}' {} \
         > "GB-Test/${element}s_training.fasta"
done
```

# Data Characterization
## Libraries required 
Not many changes had been currently done to the libraries required, 
however many are expected to be included in order to perform some 
statistical tests. Their list, and relevant descriptions are below:

\vspace{0.33cm}
\begin{rheader}
R Code
\end{rheader}
\vspace{-1.75pt}
```{r}
#| label: requirements
#| eval: true
#| output: false

# For genome-functions.R
library(stringr)
library(stringi)
library(primes)
# Data handling
library(data.table)  # Large Data Processing
library(dplyr)
library(plyr)
# For parallel computing
library(doParallel)
library(foreach)
# For biological functions: 
library(Biostrings)  # Global Alignments 
library(DNAshapeR)   # DNA Shape Features
# For plotting
library(paletteer)   # Colot Palettes
library(cowplot)     # Plot Grids
library(ggplot2)
library(see)         # Half-Violing Plots
# For pretty tables
library(knitr)       # PDF rendering options
library(kableExtra)  # Prettier tables
# For statistic analysis
library(stats)
library(irlba)       # Memory-efficient PCA
# library(nortest)
```

Then we load both our \impw{"pretty-display"} functions 
and our \impw{genomic} functions (which we've changed 
 to reduce the biases identified in the previous test).

```{r}
#| label: source-functions-facade
#| eval: false
project_path <- "/path/to/Project/"
source(paste0(project_path, "scripts/genome-functions.R"))
source(paste0(project_path, "scripts/custom-functions.R"))
```
```{r}
#| label: source-functions
#| cache: false
#| echo: false
project_path <- "/home/davidfm/Projects/EnhaProm/"
source(paste0(project_path, "scripts/genome-functions.R"))
source(paste0(project_path, "scripts/custom-functions.R"))
```

## Sequence Characterization
Note that we did not use the full extent of both '\impw{promoters}' 
and '\impw{OCRs}' sequence sets, since this allowed us to divide 
their elements evenly.
```{r}
#| label: sequence-characterization
#| eval: false
# Note: Further modifications were made to corresponding Rscript
elements <- c("enhancers", "promoters", "OCRs")
Ns_sequences <- c(85512, 75930, 69900)
# Total numbers of sequences are 85512, 75934 and 69902
Ns_elements_per_csv <- c(3563, 2531, 2330)
Ns_clusters <- c(12, 10, 10)

for (n in seq_along(elements)) {
  element_n <- elements[n]
  n_sequences <- Ns_sequences[n]
  n_elements_per_csv <- Ns_elements_per_csv[n]

  n_cycles <- n_sequences / n_elements_per_csv
  corescluster <- makeCluster(Ns_clusters[n])
  registerDoParallel(corescluster)

  element_path <- paste0(project_path,
                         "datasets/GB-Test/", element_n)
  fasta_path <- paste0(element_path, "_training.fasta")
  seqs <- scan(fasta_path, character(),
               quote = "")[seq(2, n_sequences * 2, 2)]

  libs <- c("stringr", "stringi", "primes")
  foreach(i = 1:n) %dopar% {
    required_libs(libs)
    i_start <- ((i - 1) * n_elements_per_csv) + 1
    i_final <- i * n_elements_per_csv
    if (i > 1) {
      write.table(sequences_characterizer(seqs[i_start:i_final], 
                                          Ks = c(2, 3, 5)),
                  paste0(element_path, "-training_", i, ".csv"),
                  sep = ",", row.names = FALSE, col.names = FALSE)
    } else {
      write.csv(sequences_characterizer(seqs[i_start:i_final],
                                        Ks = c(2, 3, 5)),
                paste0(element_path, "-training_", i, ".csv"),
                row.names = FALSE)
    }
  }
  stopCluster(corescluster)
}
```

This characterization process took around 3 hrs for each element (the longest set --of enhancers--, took 
3.034 hours to finish), which came to an approximate of 9 hrs of processing time in total.

## CSV Concatenation
\vspace{0.33cm}
\begin{bashheader}
Bash Code
\end{bashheader}
\vspace{-1.75pt}
```{bash}
#| label: csv-concatenation
#| eval: false
cd /path/to/Project/datasets
mkdir ./GB-Test/tabulator-generated

elements=("enhancer" "promoter" "OCR")

# Concatenate all CSVs into their respective file
for element in "${elements[@]}"; do
  ls GB-Test/"${element}"*.csv | sort -V | \
  xargs cat > GB-Test/features_"${element}"s.csv
  # Move CSVs to different directory for better readability
  mv GB-Test/"${element}"*.csv GB-Test/tabulator-generated
done
```

## Sequence Lengths
Due to the uniformity of sequence lengths (\impw{251 bp} and \impw{500 bp} for promoters and 
enhancers repectively) of the previous dataset, it was decided to take account of \impw{sequence 
length} distribution as a feature of the current dataset. However, since this was not one of the 
features with highest priority, I forgot to add it to '\fn{sequence\_characterizer}' before the 
generation of the already described CSVs. The following AWK script takes care of that:

\vspace{0.33cm}
```{bash}
#| label: get-seqlengths
#| eval: false

cd /path/to/Project/datasets/GB-Test
elements=("enhancer" "promoter" "OCR")

for element in "${elements[@]}"; do
  awk '!/^>/{print length}' "${element}"s_training.fasta \
  > seqlens_"${element}"s.txt
done
```


# Exploration analysis
## Sequence Length Distribution
Due to the uniformity of sequence lengths (\impw{251 bp} and \impw{500 bp} for promoters and 
enhancers repectively) of the previous dataset, it was decided to take account of \impw{sequence 
length} distribution as a feature of the current dataset. However, since this was not one of the 
features with highest priority, I forgot to add it to '\fn{sequence\_characterizer}' before the 
generation of the already described CSVs. The following AWK script takes care of that:

\vspace{0.33cm}
\begin{rheader}
R Code
\end{rheader}
\vspace{-1.75pt}
```{r}
#| label: seqlen-load
#| cache-lazy: false
#| cache: true

setwd(paste0(project_path, "datasets/GB-Test"))

seq_lengths <- list(
  Enhancer = scan("seqlens_enhancers.txt", sep = "\n"),
  Promoter = scan("seqlens_promoters.txt", sep = "\n"),
  OCR = scan("seqlens_OCRs.txt", sep = "\n"))
 
list_lengths <- sapply(seq_lengths, length) 

seq_lengths_df <- data.frame(
  Type = rep(names(seq_lengths), list_lengths),
  Length = unlist(seq_lengths)
)

seq_ranges_df <- seq_lengths_df %>%
  group_by(Type) %>%
  dplyr::summarise(Min = min(Length), Max = max(Length)) %>%
  dplyr::mutate(Position = row_number())

bw <- 20
max_len <- ((max(seq_lengths_df$Length) %/% bw) + 1) * bw
x_breaks <- seq(100, max_len, 100)
```

```{r}
#| label: seq-ranges-plot
#| echo: false
#| row: screen
#| fig-height: 3
#| crop: true
#| fig-align: center

ggplot(seq_ranges_df, aes(x = Min, xend = Max,
                          y = Position, color = Type)) +
  geom_segment(linewidth = 3) +
  scale_y_continuous(limits = c(0, 4),
                     breaks = seq_ranges_df$Position,
                     labels = seq_ranges_df$Type) +
  scale_color_manual(values = c("Enhancer" = "turquoise",
                                "Promoter" = "coral",
                                "OCR" = "darkolivegreen3")) +
  labs(x = "Sequence Length", y = "",
       title = "Sequence Length Ranges") +
  theme_minimal() +
  theme(legend.position = "none")

```
```{r}
#| label: seq-lens-hist
#| echo: false
#| row: screen
#| fig-height: 6
#| crop: true
#| fig-align: center

ggplot(seq_lengths_df, aes(x = Length, fill = Type)) +
  geom_histogram(binwidth = bw, alpha = 0.7,
                 position = "identity") +
  scale_x_continuous(breaks = x_breaks) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom") +
  labs(x = "Sequence Length", y = "Count", 
       title = "Sequence Lengths Histogram")
```

With this histogram we can notice that \impw{OCR} sequences follow a more "*natural*" length 
distribution, while \impw{promoters} and \impw{enhancers} seem to distribute along semi-defined 
lengths (mainly 200, 400, and 600 bp). Since sequence length distribution is fairly different in 
each set of sequences, I will opt to momentarily leave the current '\impw{sequence length}' feature
out of the principal table.

## Kmer Features' Distributions
Since we only computed **kmers** of **size** **2**, **3** and **5**, the number of \impw{per kmer}
feautures will be significantly less. This decision was taken after noticing in the previous test 
that kmers up to size 5 were still able to separate data after PCA. Now we'll make a visualization
of the computed kmers.
```{r}
#| label: kmer-sizes
#| layout-nrow: 3
#| eval: true
#| cache: false
# source("../scripts/genome-functions.R") #<--------------------------------------
# source("../scripts/custom-functions.R") #<--------------------------------------
k_Ns <- c(n_ki(2), n_ki(3), n_ki(5)) 
k_stt <- c(0); k_fin <- c(0)

for (k_n in k_Ns) {
  k_stt <- c(k_stt, tail(k_fin,1) + 1)
  k_fin <- c(k_fin, tail(k_fin,1) + k_n)
}

k_inds <- array(data = c(k_stt[-1], k_fin[-1]), dim = c(3, 2), 
                dimnames = list(1:3, c("start","final")))

kmer_set_sizes <- data.frame(sizes = k_Ns,
                             k_sets = c("2", "3", "5"))

```

```{r}
#| label: kmer-sizes-plot
#| echo: false
#| eval: true
#| crop: true
#| fig-height: 7
#| fig-align: center

library(ggplot2)
ggplot(kmer_set_sizes, aes(x = "", y = sizes,
                           fill = k_sets))+ 
  geom_bar(width = 1, stat = "identity") + 
  labs(y = "Count", x = "", 
       title = "Kmers per Kmer Size", 
       fill = "k =") +
  coord_polar("y", start = 0) +
  theme_minimal() +
  scale_fill_paletteer_d("PNWColors::Bay") +
  theme(legend.position = "right", 
        axis.title = element_text(size = 15),
        text = element_text(size = rel(4.25)),
        legend.text = element_text(size = 13),
        legend.title = element_text(size = 13.5),
        axis.text.y = element_text(family = "mono"),
        plot.title = element_text(size = 16, hjust = 0.5))
```


## Feature Means and Standard Deviations
Given the amount of data generated, '\fn{knitr}' seemed to have difficulties loading the generated 
CSVs, therefore in order to create the "Summary Table" of \impw{Means} and \impw{Std. Deviations}, 
an \impw{AWK} approach was tried in order to exploit the line-by-line processing nature of said 
language. Since the initial problem with '\fn{knitr}' was solved after disabling *LazyLoading* 
through the '\fn{cache-lazy: false}' chunk-execution Quarto option, further comparisons with R 
should be made to justify its' use instead of an all-R approach. The following AWK code was 
saved as \impw{get\_summary.awk}:

\vspace{0.33cm}
\begin{awkheader}
AWK Code
\end{awkheader}
\vspace{-1.75pt}
```{awk}
#| label: get-summary-awk
#| eval: false
BEGIN {
    FS = ","      # Input field separator (CSV format)
    OFS = ","     # Output field separator (CSV format)
    OFMT = "%.7g" # Same number of decimals as R
} 
FNR == 1 {
    if (ARGIND == 1) {
        # Capture column headers from the first file
        for (i = 1; i <= NF; i++) {
            headers[i] = $i
        }
        # Print output headers
        print "Type", "Field", "Means", "StDevs"
    }
    next  # Skip the header row of each file
} {       # ELSE, process each lines values:
    for (i = 1; i <= NF; i++) {
        sum[i] += $i         # Sum of each column
        sumsq[i] += $i * $i  # Squares' sum for each column
    }
}
ENDFILE {
    # Determine the type based on the filename
    type = (FILENAME ~ /promoter/) ? "Promoter" : \
           (FILENAME ~ /enhancer/) ? "Enhancer" : \
           (FILENAME ~ /OCR/) ? "OCR" : "Unknown"
    # FNR as in File Number of Records (i.e. rows or lines)
    count = FNR - 1
    # Output summaries for each column
    for (i = 1; i <= NF; i++) {
        mean = sum[i] / count
        stddev = sqrt((sumsq[i] / count) - (mean * mean))
        print type, headers[i], mean, stddev
    }
    # Reset data for the next file
    delete sum
    delete sumsq
}
```

Assuming we're already have our CSVs inside "\fn{/path/to/Project/}",
we have to run the last AWK script as follows:
\vspace{0.33cm}
\begin{bashheader}
Bash Code
\end{bashheader}
\vspace{-1.75pt}
```{bash}
#| label: run-get-summary
#| eval: false
cd /path/to/Project/
awk -f scripts/get_summary.awk datasets/GB-Testing/*.csv \
    > datasets/GB-Test/summary_features.csv
```

\vspace{0.33cm}
\begin{rheader}
R Code
\end{rheader}
\vspace{-1.75pt}
```{r}
#| label: data-summary
#| eval: true
#| cache-lazy: false

setwd(project_path)
# setwd("/home/davidfm/Projects/UBMI-IFC/EnhaProm/")
# CRE_summary <- read.csv("datasets/summary_features.csv",
CRE_summary <- read.csv("datasets/GB-Test/summary_features.csv",
                        check.names = FALSE)
# CRE_summary2 <- read.csv("datasets/summary_CREs.csv",
#                         check.names = FALSE)
```

```{r}
#| label: summary-head
#| eval: true
#| cache-lazy: false

nrows <- length(which(CRE_summary$Type == "Enhancer"))
coffeetable(CRE_summary[c(1:13, nrows + 1:13,
                          (2 * nrows) + 1:13), ])
```

Now, in order to work with our summary table in a cleaner way, it's better if get a better
hold of the column indexes of our "\impw{per-kmer}" features. This will help us access the 
columns of \impw{KSG-product}, \impw{barcode} with an *exponent-based* metric or barcode 
with a *prime number-based* metric for either "promoters", "enhancers" or "OCRs".

\vspace{0.25cm}
```{r}
#| label: indexes
#| eval: true

pe_arr <- function(seq_data)
  return(array(data = seq_data, dim = c(rev(k_inds)[1], 3),
    dimnames = list(1:rev(k_inds)[1], c("enha","ocr","prom"))))

borders <- c(0, 3331, 6662)
inds_feats_per_kmers <- function(n, n_ws = 19, 
                                 n_per_k = 3, total = 3331) {
  k_seq <- seq(n_ws + n, total - n_per_k + n, n_per_k)
  inds <- numeric(0)
  for (i in 0:(n_per_k - 1))
    inds <- c(inds, i * total + k_seq)
  return(inds)
}

indxs <- list(prod = pe_arr(inds_feats_per_kmers(n = 1)),
              bcds = pe_arr(inds_feats_per_kmers(n = 2)),
              bclp = pe_arr(inds_feats_per_kmers(n = 3)))

```

```{r}
#| label: figure-nucls
#| eval: true
#| fig-height: 4
#| row: screen
#| fig-align: center
nucl_percs <- c(1:4 + borders[1], 1:4 + borders[3])

pyrplot_(CRE_summary[nucl_percs,], 
         x_label = "Bases", y_breaks = seq(-1, 1, 0.2), 
         title = "Percentage per Nucleotide")
```

```{r}
#| label: figure-aligns
#| eval: true
#| cache: false
#| fig-height: 6
#| row: screen
#| fig-align: center
generate_inds <- function(base_values, borders)
  unlist(lapply(borders, function(b) base_values + b))

id_vals <- generate_inds(c(7,9), borders)
sc_vals <- generate_inds(c(8,10), borders)
kshan_vals <- generate_inds(c(11,14,17), borders)
kadiv_vals <- generate_inds(c(12,15,18), borders)
krdiv_vals <- generate_inds(c(13,16,19), borders)

ibarplot_(CRE_summary[id_vals,], 
          legend_title = "Type",
          x_label = "Global Alignment Identity Means")
ibarplot_(CRE_summary[sc_vals,], 
          legend_title = "Type",
          x_label = "Global Alignment Score Means")
ibarplot_(CRE_summary[kshan_vals,], 
          y_breaks = seq(0, 9, 1), legend_title = "Type",
          x_label = "Kmers' Shannon Entropy Means")
ibarplot_(CRE_summary[kadiv_vals,], 
          legend_title = "Type",
          x_label = '"Absolute" Kmer Diversity Means')
ibarplot_(CRE_summary[krdiv_vals,], 
          legend_title = "Type",
          x_label = '"Relative" Kmer Diversity Means')
```

```{r}
#| label: figure-kmer-feats
#| eval: true
#| fig-height: 8
#| row: screen
#| fig-align: center
aCREs <- c("enha", "prom")
kmer_names <- combi_kmers(k=2)
# kmer_names2 <- combi_kmers(k=3)[1:48]

pyrplot_(CRE_summary[indxs$prod[1:16, aCREs],], kmer_names,
         x_label = "Kmers", y_breaks = seq(-150, 150, 30), 
         title = "KSG-Product Means per Kmer")

pyrplot_(CRE_summary[indxs$bcds[1:16, aCREs],], kmer_names,
         x_label = "Kmers", y_breaks = seq(-30, 30, 5), 
         title = "Barcode Profile (Exponential) Means per Kmer")

# pyrplot_(CRE_summary[indxs$bclp[17:64, aCREs],], kmer_names2,
pyrplot_(CRE_summary[indxs$bclp[1:16, aCREs],], kmer_names,
         x_label = "Kmers", #y_breaks = seq(-30, 30, 5), 
         title = "Barcode Profile (Prime Numbers) Means per Kmer")

# summ_bclp <- CRE_summary2[indxs$bclp[17:64, aCREs],]
# summ_bclp <- CRE_summary2[indxs$bclp[1:16, aCREs],]
# summ_bclp$Means <- log(summ_bclp$Means)
# summ_bclp$StDevs <- log(summ_bclp$StDevs)

# pyrplot_(summ_bclp, kmer_names2,
# pyrplot_(summ_bclp, kmer_names,
#          x_label = "Kmers", #y_breaks = seq(-30, 30, 5), 
#          title = "Barcode Profile (Prime Numbers) Means per Kmer")

# coffeetable(CRE_summary[indxs$prod[1:5,],])
# coffeetable(CRE_summary[indxs$bcds[1:5,],])
# coffeetable(CRE_summary[indxs$bclp[1:5,],])
```

With the last table we can observe a clear difference between mean values of 
\impw{CG kmers}, however this partially disrupts the scale in the plot since 
none of the rest of kmers seem to distribute as widely. If we leave 'CG' 
kmers aside, the plot looks like this:

```{r}
#| label: figure-kmer-bclp
#| eval: true
#| fig-height: 8
#| row: screen
#| fig-align: center
pyrplot_(CRE_summary[indxs$bclp[(1:16)[-7], aCREs],], kmer_names[-7],
         x_label = "Kmers", #y_breaks = seq(-30, 30, 5), 
         title = "Barcode Profile (Prime Numbers) Means per Kmer")
```

## Primary analysis
Firstly we read our CSV tables into our conviniently named dataframes.
```{r}
#| label: path-requirements
#| cache: false
#| echo: false
#| output: false
library(data.table)
```

```{r}
#| label: data-load
#| cache: false
setwd(project_path)
CRE_data <- list(
Enhancers = fread("datasets/GB-Test/features_enhancers.csv", 
                  check.names = FALSE),
Promoters = fread("datasets/GB-Test/features_promoters.csv", 
                  check.names = FALSE),
OCRs = fread("datasets/GB-Test/features_OCRs.csv", 
             check.names = FALSE))
```


```{r}
#| label: subsetting-function
subsetDT_by <- function(data_list, column) {
  data_e <- cbind(Type = rep("Enhancer",
                             dim(data_list$Enhancers)[1]),
                  data_list$Enhancers[, ..column])
  data_p <- cbind(Type = rep("Promoter",
                             dim(data_list$Promoters)[1]),
                  data_list$Promoters[, ..column])
  data_o <- cbind(Type = rep("OCR",
                             dim(data_list$OCRs)[1]),
                  data_list$OCRs[, ..column])
  return(rbind(data_e, data_p, data_o))
}
```

```{r}
#| label: revalign-id-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 7), aes(x=r_id, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: revalign-score-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 8), aes(x=r_sc, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: kmers-shannon-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 17), aes(x=k5_shan, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: kmers-absdiv-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 18), aes(x=k5_adiv, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: kmers-reldiv-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 19), aes(x=k5_rdiv, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: ws-shan-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 6), aes(x=shan, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```


```{r}
#| label: ws-temp-density
#| echo: false
ggplot(subsetDT_by(CRE_data, 5), aes(x=temp, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

CG kmers ditributions

```{r}
#| label: CG-prod-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$prod[7]), aes(x=k2.7_prod, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r}
#| label: CG-bcds-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$bcds[7]), aes(x=k2.7_bcds, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r}
#| label: CG-bclp-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$bclp[7]), aes(x=k2.7_bclp, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

CG kmers ditributions

```{r}
#| label: GC-prod-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$prod[10]), aes(x=k2.10_prod, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r}
#| label: GC-bcds-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$bcds[10]), aes(x=k2.10_bcds, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
```{r}
#| label: GC-bclp-density
#| echo: false
ggplot(subsetDT_by(CRE_data, indxs$bclp[10]), aes(x=k2.10_bclp, fill=Type)) +
  geom_density(alpha=0.7) +
  scale_fill_manual(values = c("Enhancer" = "turquoise",
                               "Promoter" = "coral",
                               "OCR" = "darkolivegreen3")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```
